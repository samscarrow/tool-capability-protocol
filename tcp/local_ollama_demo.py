#!/usr/bin/env python3
"""
Local Ollama TCP Security Demonstration

This demonstration uses your local Ollama setup to show:
1. Complete offline operation - no external APIs needed
2. Man page enrichment using local LLM analysis
3. Enhanced TCP encoding with security intelligence
4. Transparent risk assessment with local model
5. Secure sandbox with human control
6. Privacy-first approach - all data stays local

Uses Ollama API for local LLM processing.
"""

import json
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional
from urllib.error import HTTPError, URLError
from urllib.parse import urlencode
from urllib.request import Request, urlopen

# Import TCP components
from enrichment.manpage_enricher import ManPageEnricher, PrivilegeLevel, SecurityLevel
from enrichment.risk_assessment_auditor import TransparentRiskAssessor
from enrichment.tcp_encoder import EnrichedTCPEncoder, SecurityFlags
from security.sandbox_manager import SandboxPermission, TCPSandboxManager
from security.secure_tcp_agent import SecureTCPAgent


class OllamaLLMProcessor:
    """Local LLM processor using Ollama for security analysis."""

    def __init__(
        self,
        model_name: str = "llama3.2:latest",
        base_url: str = "http://localhost:11434",
    ):
        """Initialize Ollama LLM processor."""
        self.model_name = model_name
        self.base_url = base_url
        self.available = self._check_ollama_available()

        if self.available:
            print(f"‚úÖ Ollama connected: {model_name}")
        else:
            print(f"‚ö†Ô∏è  Ollama not available - falling back to rule-based analysis")

    def _check_ollama_available(self) -> bool:
        """Check if Ollama is running and model is available."""
        try:
            # Check if Ollama is running
            req = Request(f"{self.base_url}/api/tags")
            with urlopen(req, timeout=5) as response:
                if response.status != 200:
                    return False

                data = json.loads(response.read().decode())
                models = data.get("models", [])
                available_models = [model["name"] for model in models]

                if self.model_name not in available_models:
                    print(
                        f"Model {self.model_name} not found. Available models: {available_models}"
                    )

                    # Try to pull the model
                    print(f"Attempting to pull {self.model_name}...")
                    pull_data = json.dumps({"name": self.model_name}).encode()
                    pull_req = Request(
                        f"{self.base_url}/api/pull",
                        data=pull_data,
                        headers={"Content-Type": "application/json"},
                    )

                    try:
                        with urlopen(pull_req, timeout=300) as pull_response:
                            return pull_response.status == 200
                    except:
                        return False

                return True

        except Exception as e:
            print(f"Ollama check failed: {e}")
            return False

    def analyze_command_security(
        self, command: str, man_page_content: str
    ) -> Dict[str, Any]:
        """Use local LLM to analyze command security risks."""
        if not self.available:
            return self._fallback_analysis(command, man_page_content)

        prompt = f"""
Analyze the security risk of the Unix command '{command}' based on its man page content.

Man page excerpt:
{man_page_content[:2000]}...

Please provide a JSON response with:
1. security_level: one of ["safe", "low_risk", "medium_risk", "high_risk", "critical"]
2. privilege_requirements: one of ["user", "sudo", "root"]
3. destructive_operations: list of potential destructive operations
4. security_concerns: list of specific security concerns
5. risk_score: float between 0.0 and 1.0
6. rationale: brief explanation of the assessment

Focus on:
- Can this command destroy data permanently?
- Does it require elevated privileges?
- Can it modify system files or settings?
- Does it have network access capabilities?
- Are there any dangerous options or usage patterns?

Respond only with valid JSON.
"""

        try:
            data = json.dumps(
                {
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.1,  # Low temperature for consistent analysis
                        "top_p": 0.9,
                    },
                }
            ).encode()

            req = Request(
                f"{self.base_url}/api/generate",
                data=data,
                headers={"Content-Type": "application/json"},
            )

            with urlopen(req, timeout=60) as response:
                if response.status == 200:
                    result = json.loads(response.read().decode())
                    llm_response = result.get("response", "")

                # Try to extract JSON from the response
                try:
                    # Look for JSON content between ```json and ``` or just parse directly
                    if "```json" in llm_response:
                        json_start = llm_response.find("```json") + 7
                        json_end = llm_response.find("```", json_start)
                        json_content = llm_response[json_start:json_end].strip()
                    else:
                        # Try to find JSON-like content
                        json_content = llm_response.strip()
                        if not json_content.startswith("{"):
                            # Find the first { and last }
                            start = json_content.find("{")
                            end = json_content.rfind("}") + 1
                            if start != -1 and end != 0:
                                json_content = json_content[start:end]

                    analysis = json.loads(json_content)

                    # Validate required fields
                    required_fields = [
                        "security_level",
                        "privilege_requirements",
                        "risk_score",
                    ]
                    if all(field in analysis for field in required_fields):
                        print(f"   üß† LLM analysis complete for {command}")
                        return analysis
                    else:
                        print(
                            f"   ‚ö†Ô∏è  LLM response missing required fields, using fallback"
                        )
                        return self._fallback_analysis(command, man_page_content)

                except json.JSONDecodeError as e:
                    print(f"   ‚ö†Ô∏è  LLM response not valid JSON: {e}")
                    print(f"   Raw response: {llm_response[:200]}...")
                    return self._fallback_analysis(command, man_page_content)
                else:
                    print(f"   ‚ö†Ô∏è  LLM request failed: {response.status}")
                    return self._fallback_analysis(command, man_page_content)

        except Exception as e:
            print(f"   ‚ö†Ô∏è  LLM analysis failed: {e}")
            return self._fallback_analysis(command, man_page_content)

    def _fallback_analysis(self, command: str, man_page_content: str) -> Dict[str, Any]:
        """Rule-based fallback analysis when LLM is not available."""

        # Known dangerous commands
        critical_commands = {"rm", "dd", "mkfs", "fdisk", "shred", "wipefs"}
        high_risk_commands = {"chmod", "chown", "mount", "kill", "sudo", "su"}
        medium_risk_commands = {"cp", "mv", "tar", "curl", "wget", "ssh"}

        content_lower = man_page_content.lower()

        # Determine security level
        if command in critical_commands:
            security_level = "critical"
            risk_score = 0.9
        elif command in high_risk_commands:
            security_level = "high_risk"
            risk_score = 0.7
        elif command in medium_risk_commands:
            security_level = "medium_risk"
            risk_score = 0.5
        elif any(
            word in content_lower for word in ["delete", "remove", "destroy", "format"]
        ):
            security_level = "high_risk"
            risk_score = 0.8
        else:
            security_level = "safe"
            risk_score = 0.2

        # Determine privilege requirements
        if any(
            word in content_lower for word in ["root", "superuser", "administrator"]
        ):
            privilege_requirements = "root"
        elif any(word in content_lower for word in ["sudo", "elevated"]):
            privilege_requirements = "sudo"
        else:
            privilege_requirements = "user"

        return {
            "security_level": security_level,
            "privilege_requirements": privilege_requirements,
            "destructive_operations": [],
            "security_concerns": [],
            "risk_score": risk_score,
            "rationale": f"Rule-based analysis for {command}",
        }


class LocalTCPSecurityDemo:
    """Complete TCP security demonstration using local Ollama."""

    def __init__(self, demo_dir: str = None):
        """Initialize local demo with Ollama."""
        self.demo_dir = Path(demo_dir or Path.cwd() / "local_tcp_demo")
        self.demo_dir.mkdir(parents=True, exist_ok=True)

        print("üè† LOCAL OLLAMA TCP SECURITY DEMONSTRATION")
        print("=" * 70)
        print("Privacy-first approach - all processing stays on your machine")
        print()

        # Initialize components
        self.llm = OllamaLLMProcessor()
        self.enricher = ManPageEnricher(str(self.demo_dir / "manpage_cache"))
        self.encoder = EnrichedTCPEncoder(self.enricher)
        self.risk_assessor = TransparentRiskAssessor(str(self.demo_dir / "risk_audits"))
        self.sandbox = TCPSandboxManager(
            str(self.demo_dir / "sandbox"), security_level="strict"
        )

        print("‚úÖ All components initialized locally")
        print()

    def demonstrate_local_workflow(self) -> None:
        """Demonstrate complete local workflow."""
        print("üîç LOCAL SECURITY ANALYSIS WORKFLOW")
        print("=" * 70)
        print("Processing commands with local LLM and rule-based analysis...")
        print()

        # Test commands
        test_commands = ["cat", "grep", "curl", "chmod", "rm", "dd"]

        processed_data = {}

        for command in test_commands:
            print(f"üìã Analyzing: {command}")
            print("-" * 40)

            # Step 1: Get local man page
            print("1Ô∏è‚É£ Extracting local man page...")
            man_content = self.enricher.get_local_manpage(command)

            if not man_content:
                print(f"   ‚ùå No local man page for {command}")
                continue

            print(f"   ‚úÖ Man page extracted ({len(man_content)} chars)")

            # Step 2: Local LLM analysis
            print("2Ô∏è‚É£ Local LLM security analysis...")
            llm_analysis = self.llm.analyze_command_security(command, man_content)

            security_level = SecurityLevel(llm_analysis["security_level"])
            privilege_level = PrivilegeLevel(llm_analysis["privilege_requirements"])
            risk_score = llm_analysis["risk_score"]

            print(f"   ‚úÖ Security: {security_level.value} (score: {risk_score:.2f})")
            print(f"   ‚úÖ Privileges: {privilege_level.value}")

            # Step 3: Create enhanced man page data
            print("3Ô∏è‚É£ Creating enhanced man page data...")

            # Parse basic man page structure
            base_data = self.enricher.parse_manpage_content(man_content, command)

            # Override with LLM analysis
            base_data.security_level = security_level
            base_data.privilege_requirements = privilege_level
            base_data.destructive_operations = llm_analysis.get(
                "destructive_operations", []
            )
            base_data.security_notes = llm_analysis.get("security_concerns", [])

            print(f"   ‚úÖ Enhanced data structure created")

            # Step 4: Generate enhanced TCP descriptor
            print("4Ô∏è‚É£ Generating enhanced TCP descriptor...")
            descriptor = self.encoder.encode_enhanced_tcp(command)
            binary_data = self.encoder.to_binary(descriptor)

            print(
                f"   ‚úÖ Binary: {len(binary_data)} bytes, flags: 0x{descriptor.security_flags:08x}"
            )

            # Step 5: Demonstrate naive agent understanding
            print("5Ô∏è‚É£ Naive agent binary analysis...")
            flags = descriptor.security_flags

            agent_insights = []
            if flags & (1 << SecurityFlags.CRITICAL):
                agent_insights.append("üíÄ CRITICAL RISK")
            elif flags & (1 << SecurityFlags.HIGH_RISK):
                agent_insights.append("üî¥ HIGH RISK")
            elif flags & (1 << SecurityFlags.MEDIUM_RISK):
                agent_insights.append("üü† MEDIUM RISK")
            else:
                agent_insights.append("üü¢ SAFE")

            if flags & (1 << SecurityFlags.DESTRUCTIVE):
                agent_insights.append("üí• Destructive")
            if flags & (1 << SecurityFlags.REQUIRES_ROOT):
                agent_insights.append("üîë Needs root")
            elif flags & (1 << SecurityFlags.REQUIRES_SUDO):
                agent_insights.append("üîê Needs sudo")

            print(f"   ü§ñ Agent understands: {', '.join(agent_insights)}")

            processed_data[command] = {
                "llm_analysis": llm_analysis,
                "descriptor": descriptor,
                "binary_data": binary_data,
                "man_data": base_data,
            }

            print()

        return processed_data

    def demonstrate_privacy_benefits(self) -> None:
        """Show privacy and local processing benefits."""
        print("üîí PRIVACY & LOCAL PROCESSING BENEFITS")
        print("=" * 70)

        print("‚úÖ COMPLETE PRIVACY:")
        print("   ‚Ä¢ All man page analysis done locally")
        print("   ‚Ä¢ Local LLM processing via Ollama")
        print("   ‚Ä¢ No data sent to external APIs")
        print("   ‚Ä¢ No internet connection required for analysis")
        print("   ‚Ä¢ Sensitive command information stays on your machine")
        print()

        print("‚úÖ PERFORMANCE BENEFITS:")
        print("   ‚Ä¢ No network latency for analysis")
        print("   ‚Ä¢ No API rate limits or costs")
        print("   ‚Ä¢ Consistent availability (no service outages)")
        print("   ‚Ä¢ Batch processing without external limits")
        print()

        print("‚úÖ SECURITY BENEFITS:")
        print("   ‚Ä¢ No command information leaked to third parties")
        print("   ‚Ä¢ Air-gapped operation possible")
        print("   ‚Ä¢ Complete control over analysis models")
        print("   ‚Ä¢ Audit trail stays local")
        print()

        print("‚úÖ COMPLIANCE BENEFITS:")
        print("   ‚Ä¢ Meets strict data locality requirements")
        print("   ‚Ä¢ No external data processing agreements needed")
        print("   ‚Ä¢ Full control over data retention")
        print("   ‚Ä¢ Regulatory compliance in restricted environments")

    def demonstrate_secure_agent_with_local_analysis(
        self, processed_data: Dict
    ) -> None:
        """Demonstrate secure agent using locally processed data."""
        print("ü§ñ SECURE AGENT WITH LOCAL INTELLIGENCE")
        print("=" * 70)

        # Pre-approve some tools based on local analysis
        safe_tools = []
        for command, data in processed_data.items():
            if data["llm_analysis"]["security_level"] in ["safe", "low_risk"]:
                safe_tools.append(command)

        print(f"Pre-approving {len(safe_tools)} safe tools based on local analysis...")

        for command in safe_tools:
            data = processed_data[command]
            binary_data = data["binary_data"]

            approved = self.sandbox.request_tool_approval(
                tool_name=command,
                binary_path=f"/usr/bin/{command}",
                tcp_descriptor=binary_data,
                requested_permission=SandboxPermission.EXECUTE_SAFE,
            )

            if approved:
                print(
                    f"   ‚úÖ {command} approved (local analysis: {data['llm_analysis']['security_level']})"
                )

        # Create secure agent
        agent = SecureTCPAgent("local_demo_agent", self.sandbox)

        print(
            f"\nü§ñ Agent initialized with {len(agent.capability_cache)} approved tools"
        )

        # Test agent capabilities
        print("\nAgent capability demonstration:")

        # Test safe command
        safe_commands = list(agent.capability_cache.keys())
        if safe_commands:
            test_command = safe_commands[0]
            result = agent.request_tool_execution(
                test_command,
                ["--version"],
                justification="Testing local analysis approved tool",
            )

            if result["success"]:
                print(f"   ‚úÖ {test_command} executed successfully")
            else:
                print(f"   ‚ùå {test_command} execution failed: {result.get('reason')}")

        # Test dangerous command (should be blocked)
        dangerous_result = agent.request_tool_execution(
            "rm", ["-rf", "*"], justification="Testing dangerous command blocking"
        )

        if not dangerous_result["success"]:
            print(
                f"   ‚úÖ Dangerous command correctly blocked: {dangerous_result.get('reason')}"
            )
        else:
            print(f"   ‚ùå SECURITY FAILURE: Dangerous command allowed!")

    def generate_local_demo_report(self, processed_data: Dict) -> str:
        """Generate report highlighting local processing benefits."""

        total_commands = len(processed_data)
        llm_analyzed = sum(
            1
            for data in processed_data.values()
            if "LLM analysis" in data["llm_analysis"]["rationale"]
        )

        report_lines = [
            "üè† LOCAL OLLAMA TCP SECURITY DEMONSTRATION REPORT",
            "=" * 70,
            f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"Processing: 100% Local (no external APIs)",
            "",
            "PROCESSING SUMMARY:",
            "-" * 30,
            f"Commands processed: {total_commands}",
            f"Local LLM analysis: {llm_analyzed}/{total_commands}",
            f"Fallback rule-based: {total_commands - llm_analyzed}/{total_commands}",
            f"Privacy maintained: 100%",
            "",
            "LOCAL LLM PERFORMANCE:",
            "-" * 30,
            f"Model used: {self.llm.model_name}",
            f"Connection: {self.llm.base_url}",
            f"Availability: {'‚úÖ Connected' if self.llm.available else '‚ùå Offline'}",
            "",
            "SECURITY INTELLIGENCE EMBEDDED:",
            "-" * 30,
        ]

        for command, data in processed_data.items():
            analysis = data["llm_analysis"]
            descriptor = data["descriptor"]

            report_lines.append(f"{command}:")
            report_lines.append(f"   Security: {analysis['security_level']}")
            report_lines.append(f"   Risk Score: {analysis['risk_score']:.2f}")
            report_lines.append(f"   Binary Flags: 0x{descriptor.security_flags:08x}")
            report_lines.append(f"   Size: {len(data['binary_data'])} bytes")

        report_lines.extend(
            [
                "",
                "KEY ACHIEVEMENTS:",
                "-" * 30,
                "‚úÖ Complete offline operation - no external API dependencies",
                "‚úÖ Privacy-first analysis - all data stays on your machine",
                "‚úÖ Local LLM integration via Ollama for intelligent analysis",
                "‚úÖ Enhanced TCP descriptors with embedded security intelligence",
                "‚úÖ Naive agents understand risks from binary descriptors alone",
                "‚úÖ Human-controlled sandbox with transparent audit trails",
                "‚úÖ Suitable for air-gapped and high-security environments",
                "",
                "PRIVACY & COMPLIANCE BENEFITS:",
                "-" * 30,
                "üîí Zero data leakage to external services",
                "üîí Complete control over analysis models and data",
                "üîí Meets strictest data locality requirements",
                "üîí No external dependencies for security analysis",
                "üîí Audit trails and processing remain local",
                "",
                "üè† LOCAL AI = INTELLIGENT + PRIVATE + SECURE",
            ]
        )

        return "\n".join(report_lines)


def main():
    """Run local Ollama TCP security demonstration."""
    print("üöÄ LAUNCHING LOCAL OLLAMA TCP SECURITY DEMONSTRATION")
    print("=" * 80)
    print("Demonstrating complete local processing with Ollama LLM")
    print("Privacy-first approach - all data processing stays on your machine")
    print()

    try:
        # Initialize demonstration
        demo = LocalTCPSecurityDemo()

        # Run local workflow
        processed_data = demo.demonstrate_local_workflow()

        print("\n")

        # Show privacy benefits
        demo.demonstrate_privacy_benefits()

        print("\n")

        # Demonstrate secure agent
        demo.demonstrate_secure_agent_with_local_analysis(processed_data)

        print("\n")

        # Generate and display report
        report = demo.generate_local_demo_report(processed_data)
        print(report)

        print("\nüéâ LOCAL DEMONSTRATION COMPLETE!")
        print("=" * 80)
        print("‚úÖ Privacy-first TCP security system successfully demonstrated")
        print("‚úÖ Local LLM integration with Ollama working")
        print("‚úÖ Complete offline operation - no external APIs needed")
        print("‚úÖ Enhanced security intelligence embedded in binary descriptors")
        print("‚úÖ Human control and audit trails maintained locally")
        print("\nüè† The future of private, secure AI automation!")

    except KeyboardInterrupt:
        print("\n\nüëã Demonstration interrupted by user.")
    except Exception as e:
        print(f"\n‚ùå Demonstration failed: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
